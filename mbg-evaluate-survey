# Copyright (C) 2010 Anand Patil
# 
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# 
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from optparse import OptionParser
import os, sys, time, imp, datetime

# Create option parser
req_doc = """

mbg-map  Copyright (C) 2009 Anand Patil
This program comes with ABSOLUTELY NO WARRANTY.
This is free software, and you are welcome to redistribute it under certain conditions.
See <http://www.gnu.org/licenses/> for the terms of the license.


  module                The module from which maps are to be generated.
  database-file         The name of the database file produced by the mcmc.
  burn                  The number of initial MCMC iterations to discard. 
                        You must provide this argument.
  survey                A CSV file containing the planned survey. Should have 
                        the same columns as the original input datafile that
                        can be measured or planned in advance of the survey.
  mask                  A raster file with some pixels missing. Maps will 
                        be generated in raster files with identical masks.
"""
p = OptionParser('usage: %prog module database-file burn mask [options]' + req_doc)
p.add_option('-n','--n-bins',help='The number of bins to use when creating histograms. Defaults to 100.',dest='n_bins',type='int')
p.add_option('-b','--bufsize',help='The size of the buffer to use, in pixels. Use 0 if raster-thin=1. Defaults to 0.',dest='bufsize',type='int')
p.add_option('-c','--credible-intervals',help="The centered credible interval posteriors to generate. Should be in the form '0.5 0.9', and the inverted commas are important! Defaults to '0.5 0.9'",dest='credible_intervals')
p.add_option('-q','--quantiles',help="The quantile maps to generate. Should be in the form '0.05 0.25 0.5 0.75 0.95', and the inverted commas are important! Defaults to '0.05 0.25 0.5 0.75 0.95'",dest='quantile_list')
p.add_option('-r','--raster-thin',help='The raster will be kriged at this level of degradation, then regridded. Set to 1 for final maps, ~10 for mild curiosity. Defaults to 1.',dest='raster_thin',type='int')
p.add_option('-t','--thin',help='How much to thin the MCMC trace. Defaults to 10.',dest='thin',type='int')
p.add_option('-i','--iter',help='The total number of samples to use in generating the map. Defaults to 20000',dest='total',type='int')
p.add_option('-p','--raster-path',help="The path to the covariate rasters. Defaults to the current working directory.",dest='raster_path')
p.add_option('-y','--year',help='The decimal year at which the map should be produced. Required for space-time models.',dest='year',type='float')
p.add_option('-d','--ignore-npd',help='Whether to discard and continue past non-positive-definite iterations.',dest='continue_past_npd',type='int')
p.add_option('-m','--match_moments',help='Whether to approximate the likelihood by matching posterior moments rather than minimizing KL divergence between true and approximate posterior. Defaults t 0.',dest='match_moments',type='int')

p.set_defaults(n_bins=100)
p.set_defaults(raster_path=os.getcwd())
p.set_defaults(raster_thin=1)
p.set_defaults(thin=50)
p.set_defaults(total=1000)
p.set_defaults(bufsize=0)
p.set_defaults(year=None)
p.set_defaults(continue_past_npd=0)
p.set_defaults(credible_intervals='.5 .9')
p.set_defaults(quantile_list='.05 .25 .5 .75 .95')
p.set_defaults(match_moments=0)

# Parse command-line arguments and make imports
o, args = p.parse_args()
from generic_mbg import *
check_args(args, 5)
o.module, o.hf_name, o.burn, o.survey, o.mask_name = args
o.burn = int(o.burn)
exec(delayed_import_str)
import decimal


# Load up given module and load its relevant contents
survey_plan = pl.csv2rec(o.survey)
M, hf, mod, mod_name = reload_model(o.module, o.hf_name)
nuggets, obs_labels = get_nuggets_and_obs(mod, mod_name, M)
extra_reduce_fns, extra_finalize = get_reduce_finalize(mod)
all_covariate_keys = M.covariate_keys

# Parse quantiles and CI's.
bins = np.linspace(0,1,o.n_bins+1)
q=parse_quantiles(o)
if len(o.credible_intervals) == 0:
    pass
else:
    ci = map(decimal.Decimal, o.credible_intervals.split(' '))
    half = decimal.Decimal(1)/decimal.Decimal(2)
    q = np.sort(list(set(q) | set(np.hstack([(half*(1+cii), half*(1-cii)) for cii in ci]))))
    
# Create predictive locations
unmasked, x = get_mask_t(o, hf)

# Load covariates
covariate_dict = get_covariate_dict(M, o, unmasked)

# Generate simulated datasets
postproc = [close(mod.simdata_postproc,survey_plan=survey_plan)]
survey_x = get_x_from_recarray(survey_plan)
survey_covariate_dict = dict([(k,survey_plan[k]) for k in all_covariate_keys])
print 'Simulating datasets at survey locations.'
t1 = time.time()
products = hdf5_to_samps(M,survey_x,nuggets,o.burn,o.thin,o.total, [sample_reduce], postproc, survey_covariate_dict, sample_finalize, o.continue_past_npd, joint=True)
print 'Simulated datasets generated in %s seconds.'%(time.time()-t1)
samples = products[postproc[0]]['samples']


# Make output dir and workspace hdf5 file to hold intermediate results.
init_output_dir(o, 'survey-eval')
workspace_file = tb.openFile('workspace.hdf5','w')
workspace_file.createArray('/','unmasked',unmasked)
        
# Prepare reducing functions that use the workspace file rather than memory.
reduce_fns = [mean_reduce_with_hdf(workspace_file, len(samples)+1), var_reduce_with_hdf(workspace_file, len(samples)+1)]
n_arrays = 2
if len(q)>0 or len(ci)>0:
    n_arrays += o.n_bins
    def binfn(arr,n_bins=o.n_bins):
        return np.array(arr*n_bins,dtype=int)
    hsr = histogram_reduce_with_hdf(bins, binfn, workspace_file, len(samples)+1)
    hsf = histogram_finalize(bins, q, hsr)
    reduce_fns = reduce_fns + [hsr]

# Warn if the workspace file is going to be enormous.
upper_gb = samples.shape[0]*x.shape[0]*n_arrays*4*len(mod.map_postproc)/1.e9
if upper_gb > 1:
    print '\n\nUpper-bound estimate of size of intermediate results file workspace.hdf5 is %f gigabytes. You can decrease this by mapping on a smaller region, by setting the -r / --raster-thin argument to a larger value or by decreasing the number of things you want to map (map_postproc functions).\n\n'%upper_gb


# Do the predictive simulation.
print 'Updating the posterior for each simulated dataset, and simulating on predictive raster.'
t1 = time.time()
actual_totals, log_imp_weights = hdf5_to_survey_eval(M, x, nuggets, o.burn, o.thin, o.total, reduce_fns, mod.map_postproc, covariate_dict, survey_x, samples, survey_covariate_dict, mod.survey_likelihood, survey_plan, o.match_moments, finalize=None, continue_past_npd=False)
print 'Posterior predictive simulations done in %s seconds'%(time.time()-t1)


# Finalize the summary maps for each of the simulated datasets.
print 'Finalizing the summary maps for each simulated dataset.'
t1 = time.time()
for mp in mod.map_postproc:
    
    mean_arr = getattr(workspace_file.root, mp.__name__+'_mean')
    var_arr = getattr(workspace_file.root, mp.__name__+'_var')
        
    std_arr = workspace_file.createCArray('/',mp.__name__ + '_std', shape=mean_arr.shape,atom=tb.FloatAtom(),filters=tb.Filters(complevel=1,complib='zlib'))
    std_to_mean_arr = workspace_file.createCArray('/',mp.__name__ + '_std_to_mean', shape=mean_arr.shape,atom=tb.FloatAtom(),filters=tb.Filters(complevel=1,complib='zlib'))
    
    if len(q)>0:
        hist_arr = getattr(workspace_file.root, mp.__name__+'_histogram')
        quantile_arrs = dict([(qi, workspace_file.createCArray('/',(mp.__name__ + '_quantile_%s'%qi).replace('.','__'), shape=mean_arr.shape,atom=tb.FloatAtom(),filters=tb.Filters(complevel=1,complib='zlib'))) for qi in q])
    
    if len(ci)>0:
        ci_arrs = dict([(cii, workspace_file.createCArray('/',(mp.__name__ + '_ci_%s'%cii).replace('.','__'), shape=mean_arr.shape,atom=tb.FloatAtom(),filters=tb.Filters(complevel=1,complib='zlib'))) for cii in ci])
    
    for i in xrange(mean_arr.shape[0]):
        mean_arr[i] = mean_arr[i]/actual_totals[i]
        var_arr[i] = var_arr[i]/actual_totals[i] - mean_arr[i]**2
        std_arr[i] = np.sqrt(var_arr[i])
        std_to_mean_arr[i] = std_arr[i] / mean_arr[i]
        
        if len(q)>0:
            hist_info = hsf({hsr: hist_arr[i,:]}, actual_totals[i])
            for qi in q:
                quantile_arrs[qi][i,:] = hist_info['quantile-%s'%qi]

        if len(ci)>0:
            for cii in ci:
                ci_arrs[cii][i,:] = hist_info['quantile-%s'%(half*(1+cii))]-hist_info['quantile-%s'%(half*(1-cii))]
print 'Finalizing done in %s seconds'%(time.time()-t1)

map_keys = ['mean','var','std','std_to_mean']+[('quantile-%s'%qi).replace('.','__') for qi in q]+[('ci-%s'%cii).replace('.','__') for cii in ci]

# Visualize pickiness of simulated data
exp_number_retained = np.zeros(len(samples))
mass_not_in_mode = np.zeros(len(samples))
for l in xrange(len(samples)):
    imp_weights = np.exp(log_imp_weights[:,l])
    exp_number_retained[l] = np.sum(1.-(1-imp_weights)**actual_totals[-1])
    mass_not_in_mode[l] = 1-imp_weights.max()
# Make histogram of expected number of MCMC samples retained.
# If this is close to 1, the posterior given the simulated data
# is far from the prior and the importance-resampling scheme
# will not be successful.
pl.clf()
pl.hist(exp_number_retained,50)
pl.title('Total samples = %i'%actual_totals[-1])
pl.savefig('exp_number_retained.pdf')
# Make histogram of the log of the importance mass not in the mode
# for each simulated dataset. If this is large and negative, the
# scheme will not be successful.
pl.clf()
pl.hist(np.log(mass_not_in_mode),50)
pl.title('Number of posterior samples = %i'%imp_weights.shape[0])
pl.savefig('log_mass_not_in_mode.pdf')
# Make histogram of the importance mass non in the mode for each
# simulated dataset. If this is near zero, the scheme will not be
# successful. If it is near (n-1)/n, the scheme will be very 
# successful.
pl.clf()
pl.hist(mass_not_in_mode,50)
pl.title('Number of posterior samples = %i'%imp_weights.shape[0])
pl.savefig('mass_not_in_mode.pdf')


def make_pdf(v, out_name):
    pdf_name = out_name.replace('.','__') + '.pdf'
    print 'Generating output file %s'%(pdf_name)

    lon,lat,data=vec_to_raster(v,o.mask_name,o.raster_path,out_name,unmasked)
    pl.clf()
    pl.imshow(grid_convert(data,'y+x+','y-x+'), extent=[lon.min(), lon.max(), lat.min(), lat.max()], interpolation='nearest')
    pl.colorbar()
    pl.savefig(pdf_name)


# Make accumulators to reduce over the summary maps. These can
# use main memory rather than the workspace file.
map_reduce_fns = [mean_reduce, var_reduce]
if len(q)>0:
    hsr = histogram_reduce(bins, binfn)
    hsf = histogram_finalize(bins,q,hsr,ci=ci)
    map_reduce_fns.append(hsr)

# Make finalizing functions to produce summary maps of summary
# maps.
def map_finalize(prod, n, q=q):
    mean = prod[mean_reduce] / n
    var = prod[var_reduce] / n - mean**2
    std = np.sqrt(var)
    std[np.where(var<0)]=0    
    std_to_mean = std/mean
    std_to_mean[np.where(mean==0)]=np.inf
    std_to_mean[np.where(std==0)]=0
    out = {'mean': mean, 'var': var, 'std': std, 'std-to-mean':std_to_mean}
    if len(q)>0:
        out.update(hsf(prod, n))
    return out

# Produce the summary summary maps and write out.
for f in mod.map_postproc:
    for k in map_keys:
        # Pull a summary map out of the workspace file.
        hfnode = getattr(workspace_file.root, (mp.__name__+'_'+k).replace('-','_'))
        # The last row holds the current map.
        v = hfnode[-1]
        make_pdf(v, f.__name__ + '_cur_' + k)
        # The other rows hold the simulated maps given that the
        # survey plan will be carried out.
        products = dict([(rf, None) for rf in map_reduce_fns])
        for i in xrange(hfnode.shape[0]-1):
            for rf in map_reduce_fns:
                products[rf] = rf(products[rf], hfnode[i], rf.__name__)
        mapmaps = map_finalize(products, hfnode.shape[0]-1)
        for mk,v in mapmaps.iteritems():
            make_pdf(v, f.__name__+'_'+mk+'_of_'+k)